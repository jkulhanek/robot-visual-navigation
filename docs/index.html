<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="style.css" />
    <title>Visual Navigation in Real-World Indoor Environments Using End-to-End Deep Reinforcement Learning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="This webpage accompanies the paper Visual Navigation in Real-World Indoor Environments Using End-to-End Deep Reinforcement Learning and provides a link to the official code implementation." />
    <meta name="author" content="Jonáš Kulhánek" />
    <meta name="keywords" content="deep reinforcement learning,visual navigation,mobile robots,deep learning,paper,code,arxiv" />
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133355320-1"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'UA-133355320-1');</script>
  </head>
  <body>
    <header>
      <h1>
        <span class="title-small">Visual Navigation in Real-World Indoor Environments Using End-to-End Deep Reinforcement Learning</span>
      </h1>
    </header>
    <div class="authors">
      <div class="author">
        <span class="author-name">
          <a href="https://jkulhanek.github.io/">Jonáš Kulhánek</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="http://people.ciirc.cvut.cz/~derneeri/">Erik Derner</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="http://www.robertbabuska.com/">Robert Babuška</a>
        </span>
        <span class="author-affiliation">Delft University of Technology</span>
      </div>
    </div>
    <div class="links">
      <div class="link link-paper">
        <a href="https://arxiv.org/pdf/2010.10903.pdf">Paper</a>
      </div>
      <div class="link link-github">
        <a href="https://github.com/jkulhanek/robot-visual-navigation">Code</a>
      </div>
    </div>
    <div class="video">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/L6CanVH3WSU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
      </iframe>
    </div>
    <section class="abstract">
      <h2>Abstract</h2>
      <p>
      Visual navigation is essential for many applications in robotics, from manipulation, through mobile robotics to automated driving. Deep reinforcement learning (DRL) provides an elegant map-free approach integrating image processing, localization, and planning in one module, which can be trained and therefore optimized for a given environment. However, to date, DRL-based visual navigation was validated exclusively in simulation, where the simulator provides information that is not available in the real world, <i>e.g.</i>, the robot’s position or image segmentation masks. This precludes the use of the learned policy on a real robot.
Therefore, we propose a novel approach that enables a direct deployment of the trained policy on real robots.
      </p>
      <p>We have designed visual auxiliary tasks, a tailored reward scheme, and a new powerful simulator to facilitate domain randomization. The policy is fine-tuned on images collected from real-world environments.
We have evaluated the method on a mobile robot in a real office environment. Training took ~30 hours on a single GPU. In 30 navigation experiments, the robot reached a 0.3-meter neighborhood of the goal in more than 86.7&nbsp;% of cases.
This result makes the proposed method directly applicable to tasks like mobile manipulation.
      </p>
      <div class="figure">
<img src="resources/overview.svg" alt="Architecture overview" style="width: 100%; max-width:448px; margin: 0px auto; display: block;" />
<div class="caption">Figure 1: The <strong>model architecture</strong> is similar to our previous work [<a class="citation" href="#ref-kulhanek2019vision">1</a>] with the difference of the labels for VN auxiliary tasks being the raw camera images instead of the segmentation masks, which are not readily available in the real-world environment.
</div>
</div>
    </section>
    <section class="rw-experiment">
      <h2>Real-world dataset experiment</h2>
In an office room, we used the TurtleBot 2 robot to collect a dataset of images taken at grid points with a 0.2⁢ resolution When we collected the dataset, we estimated the robot pose through odometry.
We compare our method with the PAAC algorithm and the UNREAL algorithm <i>(see the paper)</i>. The models were pre-trained in a simulated environment, however, we compare also with models that did not use any pre-training (labelled as <strong>np</strong>).
We also compare with a random agent which selects random movements, but when it reaches the target, the ground truth information is used to signal the goal.
<figure class="table">
  <figcaption>
    Table 1: In this table we show the mean success rate, the mean distance from the goal (goal distance), and the mean number of steps.
  </figcaption>
<style>
#table1 tr td:nth-child(1),
#table1 tr th:nth-child(1)
{
  text-align: left;
}
#table1 tr td:nth-child(2),
#table1 tr td:nth-child(3),
#table1 tr td:nth-child(4),
#table1 tr th:nth-child(2),
#table1 tr th:nth-child(3),
#table1 tr th:nth-child(4)
{
  text-align: center;
}
</style>
 <table id="table1">
<thead>
<tr>
<th>algorithm</th>
<th>success rate</th>
<th>goal distance (m)</th>
<th>steps on grid</th>
</tr>
</tdead>
<tbody >
<tr style="font-weight:bold">
<td><span>ours</span></td>
<td>0.936</td>
<td>0.145±0.130</td>
<td>13.489±6.286</td>
</tr>
<tr>
<td>PAAC</td>
<td>0.922</td>
<td>0.157±0.209</td>
<td>14.323±10.141</td>
</tr>
<tr>
<td>UNREAL</td>
<td>0.863</td>
<td>0.174±0.173</td>
<td>14.593±9.023</td>
</tr>
<tr class="sep">
<td>np ours</td>
<td>0.883</td>
<td>0.187±0.258</td>
<td>15.880±7.022</td>
</tr>
<tr>
<td>np PAAC</td>
<td>0.860</td>
<td>0.243±0.447</td>
<td>13.699±6.065</td>
</tr>
<tr>
<td>np UNREAL</td>
<td>0.832</td>
<td>0.224±0.358</td>
<td>15.676±6.578</td>
</tr>
<tr class="sep">
<td>random</td>
<td>0.205</td>
<td>1.467±1.109</td>
<td>147.956±88.501</td>
</tr>
<tr>
<td>shortest patd</td>
<td>–</td>
<td>0.034±0.039</td>
<td>12.595±5.743</td>
</tr>
</tbody>
 </table>
</figure>
</section>
    <section class="results-rw">
      <h2>Real-world evaluation</h2>
      <p>Finally, to evaluate the trained network in the real-world environment, we have randomly chosen 30 pairs of initial and target states. The trained robot was placed in an initial pose, and it was given a target image. The robot reached the 0.3 meter radius of the goal in 86.7% of the cases. We show a video of one of the episodes.
      </p>
      <div class="video">
        <video width="320" height="240" loop autoplay muted>
          <source src="https://data.ciirc.cvut.cz/public/projects/2021RealWorldNavigation/videos/turtlebot.mp4" type="video/mp4">
        </video>
      </div>
    </section>
    <section class="references">
      <h2>References</h2>
      <div id="ref-kulhanek2019vision">
        [1] Jonáš Kulhánek, Erik Derner, Tim De Bruin, and Robert Babuška. <i>Vision-based navigation using deep reinforcement learning.</i> In 2019 European Conference on Mobile Robots (ECMR), pages 1-8, 2019.
      </div>
    </section>
    <section class="citation">
      <h2>Citation</h2>
      <span>Please use the following citation:</span>
      <pre>
@article{kulhanek2021visual,
  title={Visual navigation in real-world indoor environments using end-to-end deep reinforcement learning},
  author={Kulh{\'a}nek, Jon{\'a}{\v{s}} and Derner, Erik and Babu{\v{s}}ka, Robert},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={3},
  pages={4345--4352},
  year={2021},
  publisher={IEEE}
}
</pre>
    </section>
<a href="https://github.com/jkulhanek/robot-visual-navigation/" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
  </body>
</html>
